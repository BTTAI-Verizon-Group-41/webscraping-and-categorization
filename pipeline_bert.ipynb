{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f13e601b",
   "metadata": {},
   "source": [
    "# Automated Website categorization using machine learning algorithms (DT)\n",
    "\n",
    "This notebook processes the website contents data and builds a BERT model to predict the category of websites.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is an advanced NLP model that provides deep contextual understanding of language. Unlike traditional models, BERT can interpret the meaning of words in context, significantly improving the accuracy of text classification tasks.\n",
    "\n",
    "# Stages of the project\n",
    "- Web Scraping: Extract textual content from websites using tools like BeautifulSoup and Selenium.\n",
    "- Data Preprocessing: Prepare and clean the text data for input into the model.\n",
    "- Modeling: Decision Tree, Regression Tree, BERT\n",
    "- Output Results: Evaluate the model performance.\n",
    "\n",
    "# Model implementation in this file\n",
    "The BERT model is implemented using the Hugging Face Transformers library. The following steps are performed:\n",
    "1. Prepare Data\n",
    "2. Preprocessing\n",
    "3. Modeling & Fine-tuning\n",
    "4. Evaluation using different metrics (e.g. accuracy, precision, recall)\n",
    "\n",
    "Verizon, Group 41\n",
    "<br>Athena Bai, Tia Zheng, Kathy Yang, Tapuwa Kabaira, Chris Smith\n",
    "\n",
    "Last updated: Dec. 1, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba941b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\miniconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\miniconda3\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\miniconda3\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Scikit-learn for evaluation metrics\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b14d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\miniconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in d:\\miniconda3\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\miniconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\miniconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\miniconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\miniconda3\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in d:\\miniconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\miniconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\miniconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\miniconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba0f16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\cosi114a\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0bf266f-fddb-4ed4-9ee3-32743b519b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dab32a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 7535: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#. Load and Preprocess Data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Assuming 'text_content' column contains website content\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdf_text.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\cosi114a\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\cosi114a\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\cosi114a\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\cosi114a\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\miniconda3\\envs\\cosi114a\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 7535: invalid start byte"
     ]
    }
   ],
   "source": [
    "#. Load and Preprocess Data\n",
    "# Assuming 'text_content' column contains website content\n",
    "data = pd.read_csv('df_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5872e-a0b4-42b8-b186-007faf8839da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis='text_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b68a5-1ef5-4d74-ba1c-9499a8ce025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef7809-e3c7-42a3-89ab-89dd62f02f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['category'] = label_encoder.fit_transform(data['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['category'] = label_encoder.fit_transform(data['category'])\n",
    "\n",
    "# 3. Custom Dataset Class for BERT\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312cd834",
   "metadata": {},
   "source": [
    "Here’s an explanation of what each part of this code is doing:\n",
    "\n",
    "### 1. **Label Encoding**\n",
    "```python\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['category'] = label_encoder.fit_transform(data['category'])\n",
    "```\n",
    "- **Purpose**: This part of the code is encoding the target labels (categories) as numerical values.\n",
    "- **Explanation**:\n",
    "  - `LabelEncoder` is a utility from `sklearn` that converts categorical labels (text) into numerical form, which is required for model training.\n",
    "  - `fit_transform` assigns a unique integer to each category in `data['category']`. For example, if categories were [\"Sports\", \"News\", \"Technology\"], they would be encoded as integers like `[0, 1, 2]`.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Custom Dataset Class for BERT**\n",
    "```python\n",
    "# 3. Custom Dataset Class for BERT\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {key: val.squeeze(0) for key, val in encoding.items()}, torch.tensor(label, dtype=torch.long)\n",
    "```\n",
    "- **Purpose**: This is a custom dataset class for preparing the text data in a format that BERT can understand. It helps in managing how data is fed to the model during training.\n",
    "- **Explanation**:\n",
    "  - `__init__(self, texts, labels, tokenizer, max_len=512)`: The constructor initializes the class with:\n",
    "    - `texts`: The list of text content (website content).\n",
    "    - `labels`: The list of numerical category labels (encoded in the previous step).\n",
    "    - `tokenizer`: The BERT tokenizer to convert text to tokens.\n",
    "    - `max_len`: The maximum length of each tokenized sequence.\n",
    "  - `__len__(self)`: Returns the number of samples in the dataset.\n",
    "  - `__getitem__(self, idx)`: This method retrieves an individual sample (text and label) and prepares it for BERT:\n",
    "    - `text = self.texts[idx]` and `label = self.labels[idx]`: Retrieve the text and corresponding label at index `idx`.\n",
    "    - `encoding = self.tokenizer(...)`: Tokenizes the text using BERT’s tokenizer with specified parameters:\n",
    "      - **`truncation=True`**: Truncates the text if it exceeds `max_len`.\n",
    "      - **`padding='max_length'`**: Pads the text to `max_len`.\n",
    "      - **`max_length=self.max_len`**: Limits the tokenized output to the specified `max_len`.\n",
    "      - **`return_tensors=\"pt\"`**: Returns the encoding as PyTorch tensors, which is the required format for BERT.\n",
    "    - `return {key: val.squeeze(0) for key, val in encoding.items()}, torch.tensor(label, dtype=torch.long)`: Returns a dictionary of the tokenized text (as PyTorch tensors) and the label as a tensor for each sample. The `squeeze(0)` is used to remove an extra dimension added by the tokenizer, making the tensors compatible for BERT.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "- **Label Encoding**: Converts categorical labels into numerical format required by the model.\n",
    "- **Custom Dataset Class**: Prepares each sample by tokenizing the text and converting both text and labels to PyTorch tensors, making it ready for BERT’s input format. This class is essential for managing and loading data in a way compatible with BERT during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and Dataset Preparation\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = TextDataset(data['text_content'].tolist(), data['category'].tolist(), tokenizer)\n",
    "\n",
    "# Train-Test Split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bafbee",
   "metadata": {},
   "source": [
    "Here’s a detailed explanation of what each part of this code is doing:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Tokenizer and Dataset Preparation**\n",
    "```python\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = TextDataset(data['text_content'].tolist(), data['category'].tolist(), tokenizer)\n",
    "```\n",
    "\n",
    "#### **BERT Tokenizer Initialization**\n",
    "- **`BertTokenizer.from_pretrained('bert-base-uncased')`**:\n",
    "  - This line loads a pretrained tokenizer for the **BERT base model** with an **uncased vocabulary** (all words are converted to lowercase).\n",
    "  - **Purpose**: The tokenizer splits input text into tokens that correspond to BERT's vocabulary, adds special tokens like `[CLS]` and `[SEP]`, and converts tokens to their corresponding IDs.\n",
    "\n",
    "#### **Dataset Initialization**\n",
    "- **`TextDataset` Class**:\n",
    "  - Initializes a dataset object that prepares text samples and labels for BERT.\n",
    "  - **Inputs**:\n",
    "    - `data['text_content'].tolist()`: A list of website text content.\n",
    "    - `data['category'].tolist()`: A list of encoded category labels.\n",
    "    - `tokenizer`: The initialized BERT tokenizer.\n",
    "  - **Purpose**: This step processes the data, ensuring that each sample is correctly tokenized and paired with its label, ready for model input.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Train-Test Split**\n",
    "```python\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "```\n",
    "\n",
    "#### **Train-Test Split Logic**\n",
    "- **`train_size = int(0.8 * len(dataset))`**:\n",
    "  - Calculates the size of the training dataset as **80%** of the total dataset.\n",
    "- **`random_split(dataset, [train_size, len(dataset) - train_size])`**:\n",
    "  - Splits the full dataset into two subsets:\n",
    "    - **`train_dataset`**: Contains 80% of the data.\n",
    "    - **`test_dataset`**: Contains the remaining 20%.\n",
    "  - **Purpose**: Ensures the model is trained on one subset (training set) and evaluated on an unseen subset (test set) to measure its generalization performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **DataLoader Initialization**\n",
    "```python\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "```\n",
    "\n",
    "#### **`DataLoader` Explanation**\n",
    "- **`DataLoader`**: Efficiently loads batches of data for training and testing.\n",
    "  - **Inputs**:\n",
    "    - `train_dataset` / `test_dataset`: The respective subsets of data.\n",
    "    - `batch_size=16`: The number of samples per batch.\n",
    "    - `shuffle=True` (for `train_loader`): Randomly shuffles the training data at the beginning of each epoch to improve model training.\n",
    "    - `shuffle=False` (for `test_loader`): Keeps the test data in the same order for consistent evaluation.\n",
    "  - **Purpose**: Loads data in smaller, manageable batches for processing by BERT. This improves memory usage and speeds up training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "1. **Tokenizer**: Converts text into token IDs that BERT understands.\n",
    "2. **Dataset**: Prepares the data (tokenized text + labels) in a format compatible with BERT.\n",
    "3. **Train-Test Split**: Divides the dataset into training and testing sets for evaluation.\n",
    "4. **DataLoader**: Loads data in batches, enabling efficient training and testing while managing memory and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e08bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load BERT and Train\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        inputs, labels = batch\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680bed93",
   "metadata": {},
   "source": [
    "Here’s a detailed explanation of this code:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Load Pretrained BERT Model**\n",
    "```python\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "```\n",
    "- **`BertForSequenceClassification`**:\n",
    "  - A specific variant of BERT designed for sequence classification tasks.\n",
    "  - **`from_pretrained('bert-base-uncased')`**: Loads a pretrained BERT model (base version with an uncased vocabulary).\n",
    "  - **`num_labels=len(label_encoder.classes_)`**: Specifies the number of output labels for the classification task based on the number of unique categories in the dataset.\n",
    "  - **Purpose**: Initializes a model capable of classifying website content into one of the predefined categories.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Set Up Optimizer**\n",
    "```python\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "```\n",
    "- **AdamW Optimizer**:\n",
    "  - A variant of the Adam optimizer that includes weight decay to improve generalization.\n",
    "  - **`model.parameters()`**: Passes the model’s parameters to the optimizer for updating during training.\n",
    "  - **`lr=1e-5`**: Sets a small learning rate to ensure gradual updates, crucial for fine-tuning BERT without disrupting its pretrained knowledge.\n",
    "  - **Purpose**: Optimizes the model’s weights to minimize the loss during training.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Device Configuration**\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "```\n",
    "- **`torch.device`**:\n",
    "  - Checks if a GPU is available (via CUDA).\n",
    "  - If available, it sets the computation to run on the GPU; otherwise, it defaults to the CPU.\n",
    "- **`model.to(device)`**:\n",
    "  - Transfers the model’s computations to the specified device for efficient processing.\n",
    "  - **Purpose**: Maximizes training efficiency, especially on large models like BERT.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Training Loop**\n",
    "```python\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        inputs, labels = batch\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "```\n",
    "\n",
    "#### **Breaking Down the Training Loop**:\n",
    "1. **`epochs = 3`**:\n",
    "   - Specifies the number of times the model will pass through the entire training dataset.\n",
    "\n",
    "2. **`model.train()`**:\n",
    "   - Sets the model to training mode, enabling it to learn and update weights.\n",
    "\n",
    "3. **`tqdm` Loop**:\n",
    "   - Wraps the training loop with a progress bar to monitor training progress and loss in real time.\n",
    "\n",
    "4. **Batch Processing**:\n",
    "   - **`for batch in loop`**: Iterates through batches of training data.\n",
    "   - **`inputs, labels = batch`**: Separates input features and labels for each batch.\n",
    "   - **`inputs = {k: v.to(device)}`**: Moves tokenized inputs to the appropriate device (CPU/GPU).\n",
    "   - **`labels = labels.to(device)`**: Moves labels to the same device.\n",
    "\n",
    "5. **Backward Pass**:\n",
    "   - **`optimizer.zero_grad()`**: Clears gradients from the previous step to prevent accumulation.\n",
    "   - **`outputs = model(**inputs, labels=labels)`**: Passes inputs through the model and computes the loss.\n",
    "   - **`loss.backward()`**: Computes gradients for all trainable parameters.\n",
    "   - **`optimizer.step()`**: Updates model weights based on computed gradients.\n",
    "\n",
    "6. **Progress Tracking**:\n",
    "   - **`loop.set_description()`**: Displays the current epoch.\n",
    "   - **`loop.set_postfix()`**: Displays the current batch loss.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "- **Model Loading**: Initializes a pretrained BERT model tailored for classification.\n",
    "- **Optimizer**: Fine-tunes BERT’s weights using AdamW.\n",
    "- **Device Usage**: Leverages GPU (if available) for faster training.\n",
    "- **Training Loop**: Iteratively trains the model, optimizing it to reduce loss while displaying progress and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b79d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Generate Classification Report\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "print(\"Accuracy:\", accuracy_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac2ec27",
   "metadata": {},
   "source": [
    "### Model Performance Evaluation Metrics\n",
    "\n",
    "When evaluating a classification model like BERT for website categorization, several metrics are used to assess its performance. Here's an explanation of the key metrics:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Accuracy**\n",
    "   **Formula**:\n",
    "   \\[\n",
    "   \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "   \\]\n",
    "   - **What It Measures**: The proportion of correctly classified instances out of all instances.\n",
    "   - **Strengths**: Provides a quick overview of model performance.\n",
    "   - **Limitations**: Not reliable for imbalanced datasets (where some categories are much more frequent than others).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Precision**\n",
    "   **Formula**:\n",
    "   \\[\n",
    "   \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "   \\]\n",
    "   - **What It Measures**: Among the instances predicted as a particular category, how many actually belong to that category.\n",
    "   - **Strengths**: Useful when the cost of false positives is high (e.g., misclassifying a category with strict rules).\n",
    "   - **Example**: If predicting \"News\" category, precision indicates how many of the predicted \"News\" websites are truly \"News\".\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Recall (Sensitivity)**\n",
    "   **Formula**:\n",
    "   \\[\n",
    "   \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "   \\]\n",
    "   - **What It Measures**: Among all actual instances of a category, how many were correctly predicted.\n",
    "   - **Strengths**: Useful when the cost of false negatives is high (e.g., missing critical categories).\n",
    "   - **Example**: For \"Health\" websites, recall tells how many actual \"Health\" websites were identified by the model.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **F1-Score**\n",
    "   **Formula**:\n",
    "   \\[\n",
    "   \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   \\]\n",
    "   - **What It Measures**: The harmonic mean of precision and recall.\n",
    "   - **Strengths**: Balances the trade-off between precision and recall, especially when dealing with imbalanced datasets.\n",
    "   - **Range**: F1-score ranges between 0 (worst) and 1 (best).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Confusion Matrix**\n",
    "   - **What It Shows**: A matrix that summarizes the counts of:\n",
    "     - **True Positives (TP)**: Correct predictions for a category.\n",
    "     - **False Positives (FP)**: Instances incorrectly predicted as a category.\n",
    "     - **False Negatives (FN)**: Instances of a category that the model missed.\n",
    "     - **True Negatives (TN)**: Instances correctly predicted as not belonging to a category.\n",
    "   - **Purpose**: Provides detailed insights into model errors for each category.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Macro vs. Micro Metrics**\n",
    "   - **Macro-Averaged Metrics**:\n",
    "     - Treats all categories equally by calculating the metric independently for each category and averaging them.\n",
    "     - Useful for evaluating overall performance across categories, regardless of class imbalance.\n",
    "   - **Micro-Averaged Metrics**:\n",
    "     - Aggregates contributions of all classes to compute the metric.\n",
    "     - More sensitive to class imbalance since it weighs metrics by the number of samples in each class.\n",
    "\n",
    "---\n",
    "\n",
    "### Use Cases of Each Metric\n",
    "- **Accuracy**: Best for balanced datasets with equal class distribution.\n",
    "- **Precision**: Critical when false positives are costly.\n",
    "- **Recall**: Important when false negatives are costly.\n",
    "- **F1-Score**: Ideal for imbalanced datasets to balance false positives and negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### Example from Model Evaluation\n",
    "After training, these metrics are calculated using the model's predictions on the test set. In Python, they are computed using:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "This generates a detailed breakdown of performance per category, helping to identify strengths and weaknesses of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041a339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
